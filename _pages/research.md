---
title: "IS2Lab - Research"
layout: textlay
excerpt: "IS2Lab -- Research"
sitemap: false
permalink: /research/
---

# Research

<!-- We seek to do cutting-edge, high-quality, and impactful research combining theory and practice. As such,  -->
<!-- We are closely collaborating with our industrial partners including Huawei, Alibaba Group, and Ant Group to solve real-world challenging problems in an effective and scalable way. Some of our recent projects are as follows. -->

<br>
*We are grateful to conduct research sponsored by NSFC, CCF and multiple industrial partners including Huawei, Alibaba Group, and Ant Group. Recently, we are mainly working on the following exciting research themes.*  

<!-- Here are some themes and techniques that we currently work on: -->
<!-- <br> -->

<!-- ### âœ… Deep Learning System Security -->
<br>

#### ***Track 1: Software Engineering for Trustworthy AI***
<!-- **[TOSEM 22, ICSE 21, TACAS 21, ISSTA 21, ASE 20, ICECCS 20, ICSE 19]: Testing, Verifying and Enhancing the Robustness of Deep Learning Models** -->

*We mean safe like nuclear safety as opposed to safe as in â€˜trust and safety' - Ilya Sutskever*

AI systems, including emerging AI models (e.g., deep neural networks and large language models), AI-based control systems (e.g., self-driving cars, robots, autonomous systems, etc) and AI-based applications (e.g., AI Chatbots, LLM agents, etc), are mostly built upon software, making it vital to ensure their trustworthiness from a software engineering perspective. In this line of research, we are working towards *a systematic testing, verification and repair framework* to evaluate, identify and fix the risks hidden in the AI models or AI-empowered systems, from different dimensions such as robustness, fairness, copyright and safety. This is crucial for stakeholders and AI-empowered industries to be aware of, manage and mitigate the risks in the new AI era.  

<!-- including novel testing metrics correlated to robustness, test case generation methods, automatic verification and repair techniques to comprehensively test, verify and enhance the robustness of deep learning models deployed in various application scenarios, e.g., image classification, object detection and NLP. -->

*Related selected publications: [CCS 25, ICSE 25, RA-L 24, TOSEM 24, ICSE 24, ISSTA 24, TDSC 24, ICSE Demo 23, ISSTA 23, S&P 22, ICSE 22, TOSEM 22, ASE 22, ICSE 21, TSE 21, TACAS 21, ISSTA 21, IJCIP, ICSE 20, ASE 20, ICECCS 20, ICSE 19]*

**Sample Work:**

[CCS 2025] Jianan Ma, Jingyi Wang\*, Qi Xuan and Zhen Wang. *Provable Repair of Deep Neural Network Defects through Pre-image Synthesis and Property Refinement*. The 32nd ACM Conference on Computer and Communications Security, Taipei, China, Oct, 2025.

[TDSC 2024] Xiangshan Gao, Xingjun Ma, Jingyi Wang, Youcheng Sun, Bo Li, Shouling Ji, Peng Cheng and Jiming Chen. *VERIFI: Towards Verifiable Federated Unlearning*, IEEE Transactions on Dependable and Secure Computing. (<font color="#dd0000">Best Paper Award Runner-Up of IEEE TDSC 2024</font>)

[S&P 2022] Jialuo Chen, Jingyi Wang\*, Tinglan Peng, Youcheng Sun, Peng Cheng, Shouling Ji, Xingjun Ma, Bo Li and Dawn Song. [*Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models*](https://arxiv.org/abs/2112.05588). 43rd IEEE Symposium on Security and Privacy, Oakland, USA, May 2022.

[TACAS 2021] Pengfei Yang, Renjue Li, Jianlin Li, Cheng-Chao Huang, Jingyi Wang, Jun Sun, Bai Xue and Lijun Zhang. *Improving Neural Network Verification through Spurious Region Guided Refinement*, 27th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, Luxembourg, Luxembourg (online), Apr 2021.

[ICSE 2021] Jingyi Wang, Jialuo Chen, Youcheng Sun, Xingjun Ma, Dongxia Wang, Jun Sun and Peng Cheng. [*RobOT: Robustness-Oriented Testing for Deep Learning Systems*](https://arxiv.org/abs/2102.05913). 43rd International Conference on Software Engineering, Madrid, Spain, May 2021.

[ICSE 2020] Peixin Zhang, Jingyi Wang\*, Jun Sun, Guoliang Dong, Xinyu Wang, Ting Dai, Xingen Wang and Jin Song Dong. *White-box Fairness Testing through Adversarial Sampling*. 42nd International Conference on Software Engineering, Seoul, South Korea (online), Oct 2020. (<font color="#dd0000">ACM SIGSOFT Distinguished Paper Award, ACM SIGSOFT Research Highlights.</font>)



<!-- ![]({{ site.url }}{{ site.baseurl }}/images/respic/robust.png){: style="width: 700px; float: center; margin: 0px  10px"} -->
<!-- <br> -->
<br>

<!-- #### ***Track 2: AI-Empowered Formal Design and Analysis of Security Protocols*** -->
<!-- **[TOSEM 22, ICSE 21, TACAS 21, ISSTA 21, ASE 20, ICECCS 20, ICSE 19]: Testing, Verifying and Enhancing the Robustness of Deep Learning Models** -->

<!-- Software is the core driving force for the digital operation of industrial safety-critical systems (industrial control systems, autonomous systems, etc). It is thus crucial to formally verify their software foundations (e.g., OS kernel, compilers, security protocols or control programs) for industrial safety-critical systems. In this line of research, we are working on *developing new logical foundations and specifications to better model, test and verify the desired security properties in different system software layers (especially those commonly used in safety-critical industries).*    -->
<!-- We are building systematic methodologies and toolkits including novel testing metrics correlated to robustness, test case generation methods, automatic verification and repair techniques to comprehensively test, verify and enhance the robustness of deep learning models deployed in various application scenarios, e.g., image classification, object detection and NLP. -->

<!-- *Related publications: [TSE 24, AsiaCCS/CPSS 24, TSE 23, CCS 23, CONFEST/FMICS 23, FITEE 22, IoT 22, TSE 21, ICSE 18, DSN 18, STTT 18, FM 18, FASE 17, FM 16]* -->

<!-- Sample Work: -->

<!-- ![]({{ site.url }}{{ site.baseurl }}/images/respic/robust.png){: style="width: 700px; float: center; margin: 0px  10px"} -->
<!-- <br> -->
<!-- <br> -->


#### ***Track 2: AI-Empowered Formal Analysis of System or Software Security***

*The job of formal methods is to elucidate the assumptions upon which formal correctness depends - Tony Hoare*

*Formal methods can be incorporated throughout the development process to reduce the prevalence of multiple categories of vulnerabilities - Back to the Building Blocks*

Software stack is the core driving force behind the digital operation of industrial safety-critical systems (industrial control systems, autonomous systems, etc). It is thus of paramount importance to formally verify and analyze the correctness and security of their foundational software stack, such as OS kernel, compiler, security protocol and control program, for industrial safety-critical systems. In this line of research, we are working on *developing new AI-empowered logical foundations and toolkits to better model, test, verify, monitor and enforce the desired properties and behaviors for different software layers (especially those commonly used in safety-critical industries).*   
<!-- We are building systematic methodologies and toolkits including novel testing metrics correlated to robustness, test case generation methods, automatic verification and repair techniques to comprehensively test, verify and enhance the robustness of deep learning models deployed in various application scenarios, e.g., image classification, object detection and NLP. -->

*Related publications: [ICSE 25, WWW 25, TSE 24, AsiaCCS/CPSS 24, TSE 23, CCS 23, CONFEST/FMICS 23, FITEE 22, IoT 22, TSE 18, ICSE 18, DSN 18, STTT 18, FM 18, FASE 17, FM 16]*

**Sample Work:**

[CCS 2025] Huan Sun, David Sanan, Jingyi Wang\*, Yongwang Zhao, Jun Sun and Wenhai Wang. *Generalized Security-Preserving Refinement for Concurrent Systems*. The 32nd ACM Conference on Computer and Communications Security, Taipei, China, Oct, 2025.

[ICSE 2025] Ziyu Mao, Jingyi Wang\*, Jun Sun, Shengchao Qin and Jiawen Xiong. *LLM-aided Automatic Modelling for Security Protocol Verification*. 47th International Conference on Software Engineering, Ottawa, Canada, Apr, 2025.

[WWW 2025] Xinyao Xu, Ziyu Mao, Jianzhong Su, Xingwei Lin, David Basin, Jun Sun and Jingyi Wang\*. *Quantitative Runtime Monitoring of Ethereum Transaction Attacks*. The Web Conference, Sydney, Australia, Apr, 2025.

[TSE 2023] Kun Wang, Jingyi Wang\*, Christopher M. Poskitt, Xiangxiang Chen, Jun Sun, and Peng Cheng. *K-ST: A Formal Executable Semantics of the Structured Text Language for PLCs*. IEEE Transactions on Software Engineering, 2023.

[TSE 2018] Jingyi Wang, Jun Sun, Shengchao Qin and Cyrille Jegourel. *Automatically â€˜Verifyingâ€™ Discrete-Time Complex Systems through Learning, Abstraction and Refinement*, IEEE Transactions on Software Engineering, 2018.

[ICSE 2018] Xinyu Wang, Jun Sun, Zhenbang Chen, Peixin Zhang, Jingyi Wang\* and Yun Lin. *Towards Optimal Concolic Testing*, 40th International Conference on Software Engineering, Gothenburg, Sweden, May 2018. (<font color="#dd0000">ACM SIGSOFT Distinguished Paper Award</font>)





<!-- #### ***Theme 3: AI-assisted Model Driven Engineering*** -->


<!-- ##### ðŸ˜Š **[ISSTA 23, ICSE 22, TSE 21, ICSE 20]: Testing, Interpreting and Mitigating the Hidden Bias in Deep Learning**

We are building systematic fairness testing methodologies and toolkits specially designed for efficiently uncover, inteprete and mitigate various kinds of bias, e.g., individual discrimination and group discrimination, in deep learning models deployed in Alibaba's recommender system serving millions of people.

![]({{ site.url }}{{ site.baseurl }}/images/respic/fairness.png){: style="width: 800px; float: center; margin: 0px  10px"}
<br>
<br>
 -->
<!-- ##### ðŸ˜Š **[ICSE 23, S&P 22]: Copyright Protection for Deep Learning Models**
We are building a copyright protection framework for deep learning models based on systematic testing, aiming to prove accurate and robust model copyright verification.

![]({{ site.url }}{{ site.baseurl }}/images/respic/copyright_.png){: style="width: 700px; float: center; margin: 0px  10px"}
 -->



<!-- ### âœ… Cyber-physical System Security
<br>

##### ðŸ˜Š **[JCST 21, IoT 22]: Proactive Defense for Industrial Control Systems**

![]({{ site.url }}{{ site.baseurl }}/images/respic/copyright_.png){: style="width: 700px; float: center; margin: 0px  10px"}
<br>

We are building a copyright protection framework for deep learning models based on systematic testing, aiming to prove accurate and robust model copyright verifications.


##### ðŸ˜Š **[TSE in submission]: Formal Semantics for Industrial Control Languages**

![]({{ site.url }}{{ site.baseurl }}/images/respic/copyright_.png){: style="width: 700px; float: center; margin: 0px  10px"}
<br>

We are building a copyright protection framework for deep learning models based on systematic testing, aiming to prove accurate and robust model copyright verifications.


<br> -->




<!-- ![]({{ site.url }}{{ site.baseurl }}/images/respic/nnrepair_.png){: style="width: 700px; float: center; margin: 0px  10px"}
<br>
**NN Repair.** We are building a NN(especially RNN) repair framework, aiming to repair incorrect behaviors provably.

![]({{ site.url }}{{ site.baseurl }}/images/respic/ODsystem_.png){: style="width: 800px; float: center; margin: 0px  10px"}
<br>
**Object Detection System Security Testing.** By generating multiple test cases to expose the security vulnerablities of the object detection system, and designing test metrics to evaluate the security and robustness of the object detection system, finally we can build more security and more robust OD systems by retraining.

![]({{ site.url }}{{ site.baseurl }}/images/respic/recommender_.png){: style="width: 800px; float: center; margin: 0px  10px"}
<br>
**DL Recommender System Fairness Testing:** We are building a DL recommender system testing framework to identify unfairness and find disadvantaged groups to improve the fairness of the original model.

![]({{ site.url }}{{ site.baseurl }}/images/respic/robustness_.png){: style="width: 800px; float: center; margin: 0px  10px"}
<br>
**DL Robustness Testing.** We are building a DL testing framework that aims to enhance the model robustness against various attacks in a one-step way.

![]({{ site.url }}{{ site.baseurl }}/images/respic/unlearning_.png){: style="width: 800px; float: center; margin: 0px  10px"}
<br>
**Certifiable Machine Unlearning.** We are building a certifiable machine unlearning framework that aims to guarantee the data to be forgotten and not damage model's performance. -->


#### ... and more.
